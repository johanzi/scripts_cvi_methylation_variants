---
title: "High impact mutations drive DNA methylation variation after colonization of a novel habitat"
author: "Johan Zicola"
date: "`r format(Sys.time())`"
output:
  html_notebook:
    toc: true
  pdf_document:
    toc: true
  github_document:
    toc: true
  html_document:
    toc: true
    df_print: paged
documentclass: article
classoption: a4paper
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

```{r,echo=FALSE,message=FALSE,warning=FALSE}
# Set so that long lines in R will be wrapped:
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
```


```{r}
# Source
source("scripts/functions_methylkit.R")
```


# Overview

This documentation explains step by step how was performed the analysis on whole-genome bisulfite sequencing (WGBS) data on African *Arabidopsis thaliana* accessions (Morocco and Cape Verde) and the reanalysis of a subset of the WGBS data from the 1001 Genome Project (1001GP) ([Kawakatsu et al., 2016](http://www.sciencedirect.com/science/article/pii/S0092867416308522)).

# Library preparation

Libraries were prepared as described previously in [Urich et al. 2012](http://www.nature.com/nprot/journal/v10/n3/full/nprot.2014.114.html) with minor modifications.

# Sequencing

Libraries were pooled based on the 24 NEXTFlex Bisulfite-Seq barcodes (BiooScientific) for multiplex sequencing on the HiSeq3000 sequencer (Illumina) in 150 bp single-end mode.  1 Gb (7 M reads) of data were ordered for each library (minimum required by the sequencing facility). Reads were trimmed from adapters by the sequencing facility using Cutadapt ([Martin et al., 2011](http://journal.embnet.org/index.php/embnetjournal/article/view/200)). Visual inspection on graphics produced by [fastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) was used to visually determine the quality of the reads.

# Softwares required

* Bismark (v0.19.0)
* Python3.5
* GEMMA (v0.94)
* vcftools (v0.1.14)
* bcftools (v1.2)
* bwa (v0.7.15)
* R (>3.3.0) with the libraries indicated in the scripts
* [SRA tool kit](https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=toolkit_doc) from NCBI
* FastQC v0.11.9
* multiqc v1.6
* cutadapt v2.9
* methylKit (v1.14.2) R package and dependencies

# Reanalysis of the 1001 GP data


## Get information samples

Working directory: `/srv/netscratch/dep_coupland/grp_hancock/johan/bs-seq_data_1001`

Go to NCBI SRA selector for project PRJNA187927 (SALK La Jolla) and PRJNA236110 (GMI Vienna)
Web: https://www.ncbi.nlm.nih.gov/Traces/study/?go=home enter PRJNA187927, Download the SraRunTable.txt (478 samples)

`mv SraRunTable.txt SraRunTable_SALK.txt`

Web: https://www.ncbi.nlm.nih.gov/Traces/study/?go=home enter PRJNA236110, Download the SraRunTable.txt (2215 samples)

`mv SraRunTable.txt SraRunTable_GMI.txt`

Go to http://1001genomes.org/accessions.html and download the csv file (link at the bottom of the page)

Convert CSV to tab-separated file

```{bash}
sed 's/,/\t/g' query.txt > accessions_1001genome.txt
```

A subset of 526 accessions were selected, discarding accessions from USA and replicates. A dataframe containing details about each accessions was created and put in `accessions_1001GP_figure1.txt`.

## Download data

Select SRR names of each the 526 accessions to download

```{bash}
cut -f2 accessions_1001GP_figure1.txt > to_download.txt
```

The script `download_sra.sh` retrieves fastq file for each SRR number.

```{bash}
i=$1

if [[ ! -e ${i}.sra ]]; then
  first_6_chars=$(echo $i | cut -c1-6)
  accession="${i%.*}"
  
  # Download SRA file
  echo "wget ftp://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/${first_6_chars}/${accession}/${accession}.sra"
  wget ftp://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/${first_6_chars}/${accession}/${accession}.sra
  
  # Extract fastq file from SRA
  echo "fastq-dump --split-3 ${i}.sra"
  fastq-dump --split-3 ${i}.sra
  
  # Compress fastq file(s) (1 or 2 files for SE or PE libraries, respectively)
  gzip ${i}*.fastq
  
  # Remove SRA file
  rm ${i}.sra
else
  echo "${i}.sra already exists"
fi

```

Download the data

```{bash}

# Launch script in bsub
while read i; do
  bash download_sra.sh $i
done < to_download.txt

```


## Mapping with Bismark

Reads were mapped on *A. thaliana* TAIR10 reference [fasta file](https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_chromosome_files/TAIR10_chr_all.fas). Note that the fasta file contains the 5 chromosomes and the 2 plastids (chloroplast and mitochondria). In order to be processed by the R package methylKit ([Akalin et al., 2011](https://doi.org/10.1186/gb-2012-13-10-r87)), the cytosine report files from Bismark were generated for each chromosome and each methylation context. In order to perform the analysis on many accessions, the bash script [run_bismark.sh](scripts/run_bismark.sh) performs Bismark analysis step-by-step.

Note that absolute paths can be given, the output files will be generated in the specified output directory (argument `-o`) or by default in the directory containing the input fastq file if no -o argument is specified. While running, the script will echo each step performed, which can be redirected to a log file.

The script will take care of:
* Building the bismark reference genome
* Perform the alignment
* Remove duplicate reads from the bam file 
* Extract the methylation status and generate coverage and bedGraph files (visualization in SeqMonk and IGV)
* Generate cytosine reports (used as input file for methylKit R package)
* Calculate conversion efficiency (based on spurious non-converted cytosines from the chloroplast genome) 

To get more information on how run_bismark.sh is working:

```{bash}
bash run_bismark.sh -h
```
The code itself contains comments for each step so have look at it and tweak it to your needs.


### Get reference genome

Download the fasta file for *A. thaliana* TAIR10 reference:

```{bash}
# Download fasta file
wget https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_chromosome_files/TAIR10_chr_all.fas

# Rename chromosomes to add prefix "Chr" (easier to retrieve information, e.g. Chr1 rather than 1)
sed -i 's/^>\([1-5]\)/>Chr\1/g' TAIR10_chr_all.fas 

# Move the file in "/path/to/dir_fasta/"
mv TAIR10_chr_all.fas /path/to/dir_fasta/

```



### Mapping single-end data

The data from the 1001 GP have a mix of SE and PE data, put fastq files in separate folders.

```{bash}
# Split SE and PE data in different folders
mkdir PE_data
mkdir SE_data

# Move paired-end data in PE_data
mv *_1.fastq.gz PE_data/
mv *_2.fastq.gz PE_data/

# Move the rest (SE data) into SE_data
mv *fastq.gz SE_data

```

Map the reads with the light mode on (remove unnecessary intermediary files) with the flag `-l`

```{bash}
while read i; do
bash run_all_bismark.sh -l -r </path/to/dir_fasta/> -1 ${i} -o </path/to/output/> 
done < <(ls *fastq.gz)

```

#### Assess mapping efficiency

```{bash}

for i in *bismark_bt2_SE_report.txt; do
  library=$(echo $i | cut -d'_' -f1,2)
  map=$(grep "Mapping efficiency" $i | cut -d':' -f2 -)
  echo -e "${library}\t${map}" >> mapping_efficiency.txt
done
```

Put data in excel and calculate average and SD
average: 64.89%
Stdev: 9.34%

#### Assess conversion efficiency

```{bash}
for i in *_report.conversion_efficiency.txt; do 
  name=$(echo $i | cut -d'_' -f1,2) 
  line=$(tail -n1 $i | cut -f3)
  echo $name $line
done >> conversion_efficiency.txt
```

Put data in excel and calculate average and SD

average: 99.52%
Stdev: 0.38%


### Mapping paired-end data

```{bash}

# Get single name for each pair data
ls *fastq.gz | cut -d'_' -f1,2 | uniq > list_fastq_files.txt

while read i; do
          fastq1=${i}_1.fastq.gz
          fastq2=${i}_2.fastq.gz
	        bash run_all_bismark.sh -l -r </path/to/dir_fasta/> -1 $fastq1 -2 $fastq2 -o </path/to/output/> 
done < list_fastq_files.txt

```

#### Assess mapping efficiency

```{bash}

cd /srv/netscratch/dep_coupland/grp_hancock/johan/bs-seq_data_1001/fastq_files/1001/PE_data

for i in *bismark_bt2_PE_report.txt.gz; do
  library=$(echo $i | cut -d'_' -f1,2)
  map=$(zgrep "Mapping efficiency" $i | cut -d':' -f2 -)
  echo -e "${library}\t${map}" >> mapping_efficiency.txt
done
```

average: 51.82%
Stdev: 8.21%

Interestingly, the SE end data map at higher efficiency than PE data (about 10% more uniquely mapped reads) and I observed the same trend for the data of project GC_4050. This is probably due to the fact that more reads are unlinked in SE mode and therefore reads mapping in repetitive regions in PE mode are 2 times more numerous as they belong to the same DNA fragment.

#### Assess conversion efficiency

```{bash}
for i in *_report.conversion_efficiency.txt; do 
  name=$(echo $i | cut -d'_' -f1,2) 
  line=$(tail -n1 $i | cut -f3)
  echo $name $line
done >> conversion_efficiency.txt
```

# Analysis of the accessions from Cape Verde and Morocco

We generated WGBS data for 83 accessions from Cape Verde - Santo Antao, 20 from Morocco and diverse mutants (ARABIDILLO-1 and CMT2). We also included as control the accessions Col-0, Col-3, Doer-10, and UKID116. The data for the fastq files for these samples can be downloaded in the NCBI depository PRJNA612437.

## Download fastq files

1. Go on website https://www.ncbi.nlm.nih.gov/sra and type PRJNA612437.
2. Click to "SRA Experiments"
3. Click on "Send Results to Run selector"
4. Download the SRR list by clicking 'Accession List' as SRR_Acc_List.txt

*NB: The data are all single-end reads*

In bash, download SRA files and convert them in fastq files:

```{bash}
while read name in list; do
	fastq-dump --split-spot $name
done < SRR_Acc_List.txt
```

## Rename fastq files

Change the SRR name to the name of the library

```{bash}
TODO when the data of CPV paper will be in NCBI
Also group needs to agree on name system for fastq files
```

## Run Bismark

For each fastq file, run the following command:

```{bash}
bash run_bismark.sh -1 <filename.fastq> -r </path/to/dir_fasta/> -o </name/output/directory/>
``` 


# Methylation call and DMR

Once Bismark has been run, cytosine report files for each methylation contexts are imported in methylKit R packages for further analysis.

Typical format of the bismark output file imported in methylKit:

The suffix of the file is `*_bismark_bt2.deduplicated.bismark.cov.gz.CHG_report_only_chr.txt` so it contains all the call for cytosines in CHG context, excluding the calls in organelles (chloroplast and mitochondria).

```
Chr4    1004    +       1       1       CHG     CAG
Chr4    1006    -       2       3       CHG     CTG
Chr4    1009    -       0       5       CHG     CCG
Chr4    1020    +       1       3       CHG     CCG
Chr4    1023    -       4       5       CHG     CCG
Chr4    1052    -       9       3       CHG     CCG
Chr4    1071    +       3       4       CHG     CAG
Chr4    1073    -       6       6       CHG     CTG
Chr4    1090    +       4       5       CHG     CCG
Chr4    1096    +       4       5       CHG     CTG

```

The columns represent chromosome, base position, strand position of the cytosine, number of methylated Cs, number unmethylated Cs, methylation context, and trinucleotide context.

Different functions were created to run the functions of methylKit in batch. These functions can be found in [functions_methylkit.R](functions_methylkit.R).


Here is described the pipeline used to process the methylation data in methylKit.

## Annotation Araport11

We need the last annotation of genes and TEs for Col-0. Use Araport11 annotation

Files downloaded from https://www.arabidopsis.org/download/index-auto.jsp?dir=%2Fdownload_files%2FGenes%2FAraport11_genome_release

```{bash}
# Get genes
grep -P "\tgene\t" Araport11_GFF3_genes_transposons.201606.gff  > Araport11_GFF3_genes_only.gff

# Convert gff to bed format
gff2bed < Araport11_GFF3_genes_only.gff > Araport11_GFF3_genes_only_full.bed

# Keep only first 4 columns
cut -f1,2,3,4 Araport11_GFF3_genes_only_full.bed > Araport11_GFF3_genes_only.bed

# For TEs
grep "transposable_element" Araport11_GFF3_genes_transposons.201606.gff | wc -l
35090

# In comparison, there were 35082 transposable_element feature in TAIR10 annotation

grep "transposable_element" Araport11_GFF3_genes_transposons.201606.gff  > Araport11_GFF3_transposons.gff

# Convert gff to bed format
gff2bed < Araport11_GFF3_transposons.gff > Araport11_GFF3_transposons_full.bed

# Keep only first 4 columns
cut -f1,2,3,4 Araport11_GFF3_transposons_full.bed > Araport11_GFF3_transposons.bed


# Keep TEs bigger than 4 kb
cat Araport11_GFF3_transposons.bed | awk -F'\t' '$3-$2 >= 4000 {print $0}' > Araport11_GFF3_transposons_longer_4kb.bed

# Keep TEs smaller than 500 bp
cat Araport11_GFF3_transposons.bed | awk -F'\t' '$3-$2 < 500 {print $0}' > Araport11_GFF3_transposons_smaller_500bp.bed

```



```{r,  message = FALSE}
####################################
#       Libraries and functions    #
####################################

# Load the R script functions_methylkit.R which contains wrap up functions to run in batch several
# methylKit functions
source("functions_methylkit.R")

####################################
#      Paths to DB directories     #
####################################

# Paths to database for the output files of methylKit
path_DB_CpG <- "/path/to/methylDB_CpG"
path_DB_CHG <- "/path/to/methylDB_CHG"
path_DB_CHH <- "/path/to/methylDB_CHH"

# Create a list of these 3 paths
list_DB_paths <- list(path_DB_CpG, path_DB_CHG, path_DB_CHH, path_DB_CX)

# Path containing cytosine report and bam files from bismark pipeline
path_bismark_files <- paste("/path/to/bismark/output/files/", sep = "")

####################################################
################# BED FILES ########################
####################################################

# Path to bed files for region analysis
bed_genes <- "Araport11_GFF3_genes_only.bed"

# Get coordinates of the genes body methylated and body methylated + intermediate methylated from Takuno et al., 2017 (https://academic.oup.com/mbe/article/34/6/1479/3059954)
# List sent by Takuno on 2019-06-24
bed_genes_BM <- "BM_gene_ID.bed"
bed_genes_BM_IM <- "BM_IM_gene_ID.bed"

# Analysis on cluster 5 and 6
# # Path to bed files for region analysis (see section 'Analysis of cluster 5 and 6' for details of these bed files)
bed_genes_cluster5 <- paste(path_bed, "cluster5_coordinates.bed", sep = "")
bed_genes_cluster6 <- paste(path_bed, "cluster6_coordinates.bed", sep = "")

# bed_genes_annotate <- paste(workdir, "Arabidopsis_thaliana.TAIR10.39.bed", sep="") # Version that was made from GTF (works with readTranscriptFeatures)

# All TEs
bed_TEs <- paste(path_bed, "Araport11_GFF3_transposons.bed", sep = "")

# TEs longer than 4 kb
bed_TEs_4kb <- paste(path_bed, "Araport11_GFF3_transposons_longer_4kb.bed", sep = "")

# TEs shorter than 500 bp
bed_TEs_500bp <- paste(path_bed, "Araport11_GFF3_transposons_smaller_500bp.bed", sep = "")


####################################################
################# ACCESSIONS FILES ########################
####################################################

# Path to file with accession information (several were used in the different analyses and they are all available in GitHub)
path_df_accessions <- "df_accessions.txt"

# Get information of the accessions and generate a table
# Order first the file to export so that the elements are ordered as the fastq files (3542_AA, 3542_AB, ...)
df_accessions <- read.table(path_df_accessions, header = TRUE, stringsAsFactors = TRUE)

# Order the accession as list.files() list the bismark cytosine report files
df_accessions <- order_df_accessions(df_accessions)

# I need to create an hybrid name otherwise the loading of the file won't respect the original order of the input bismark file
df_accessions$sample <- paste(df_accessions$library, df_accessions$name, sep = "_")

# Make a list of samples
list_samples <- as.list(as.vector(df_accessions$sample))

# Get list of treatments and reformat so that the first treatment is 0 (control should be 0 optimally)
# Here I put as example CMT2 allele but the variable used as treatment differ in different analysis
list_treatments <- as.vector(df_accessions$cmt2_allele)

# Change char to numeric to avoid bugs when using unite() function
list_treatments <- as.numeric(list_treatments)

# Vector of the 3 contexts analyzed
context <- c("CpG", "CHG", "CHH")
```


# Create methylKit objects

## Create methylRawListDB objects

The function `import_bismark_cytosine_report` will retrieve automatically the different cytosine report files generated by Bismark and will create flat database files, allowing to reduce RAM usage. 

```{r}
import_bismark_cytosine_report(path_bismark_files, list_DB_paths, list_samples, list_treatments)
```

## Load methylRawListDB objects

Once created, load methylRawListDB objects. The files won't actually be loaded but accessed in real time when needed.

```{r}
list_methylRawLists <- load_methylRawListDB(list_DB_paths, type = "", list_samples, list_treatments)
```

## Filter methylRawList raw

Keep only cytosine positions that have a define minimum coverage. This threshold is usually set at around 5 in most WGBS analyses but since our samples were sequenced at the minimum depth allowed by the sequencing facility, we defined a lower threshold (minimum 2). This approach is valid considering that we look at pattern across large genomic regions. We assumed we would catch any strong signal if any.

```{r}
filter_methylRawList(list_methylRawLists_raw)
```

## Load filtered methylRawListDB objects

```{r,  message = FALSE}
list_methylRawLists <- load_methylRawListDB(list_DB_paths, type = "filtered", list_samples, list_treatments)
```


# Subset genomic regions

We want now to analyze methylation patterns is specific genomic regions. For this, we need to subset our data and generate new DB flat files for the different regions.

## Subset data

```{r,  message = FALSE}
# Create subset for methylRawList
subset_methylObject(list_methylRawLists, list_DB_paths, bed_genes, "genes", "methylRaw")

subset_methylObject(list_methylRawLists, list_DB_paths, bed_TEs, "TEs", "methylRaw")

subset_methylObject(list_methylRawLists, list_DB_paths, bed_TEs_4kb, "TEs_4kb", "methylRaw")

subset_methylObject(list_methylRawLists, list_DB_paths, bed_genes_BM, "genes_BM", "methylRaw")

subset_methylObject(list_methylRawLists, list_DB_paths, bed_genes_BM_IM, "genes_BM_IM", "methylRaw")

subset_methylObject(list_methylRawLists, list_DB_paths, bed_TEs_500bp, "TEs_500bp", "methylRaw")

subset_methylObject(list_methylRawLists, list_DB_paths, bed_genes_cluster5, "genes_cluster5", "methylRaw")

subset_methylObject(list_methylRawLists, list_DB_paths, bed_genes_cluster6, "genes_cluster6", "methylRaw")
```

## Load methylRaw subset data
```{r}
# Load subset data

# Load methylRawListDB objects (without filtering)
list_methylRawLists_genes <- load_methylRawListDB(list_DB_paths, type = "genes", list_samples, list_treatments)

list_methylRawLists_TEs <- load_methylRawListDB(list_DB_paths, type = "TEs", list_samples, list_treatments)

list_methylRawLists_TEs_4kb <- load_methylRawListDB(list_DB_paths, type = "TEs_4kb", list_samples, list_treatments)

list_methylRawLists_TEs_500bp <- load_methylRawListDB_wo_CX(list_DB_paths, type = "TEs_500bp", list_samples, list_treatments)

list_methylRawLists_genes_BM <- load_methylRawListDB(list_DB_paths, type = "genes_BM", list_samples, list_treatments)

list_methylRawLists_genes_BM_IM <- load_methylRawListDB(list_DB_paths, type = "genes_BM_IM", list_samples, list_treatments)
```


# Methylation levels

We want first to visualize the methylation levels in different genomic regions. For this, we extract the weighted methylation levels

## Whole genome
```{r, fig.width=14, fig.height=5}
df_name <- "df_mean_filtered"
title <- "Weighted Methylation Level for genes"

get_df_wml(list_methylRawLists, path_DB, df_name)

load_df_wml(path_DB, df_name)

# Plot (use get() to pass the string name of the dataframe as a R object)
ggplot_all(get(df_name), title = title)
```


## Genes

```{r, fig.width=14, fig.height=5}
df_name <- "df_mean_genes"
title <- "Weighted Methylation Level for genes"

get_df_wml(list_methylRawLists_genes, path_DB, df_name)

load_df_wml(path_DB, df_name)

# Plot (use get() to pass the string name of the dataframe as a R object)
ggplot_all(df_mean_genes, title = title)
```

## All TEs

```{r, fig.width=14, fig.height=5}
df_name <- "df_mean_TEs"
title <- "Weighted Methylation Level for long TEs (>4 kb)"

get_df_wml(list_methylRawLists_TEs, path_DB, df_name)

load_df_wml(path_DB, df_name)
```

## Long TEs

```{r, fig.width=14, fig.height=5}
df_name <- "df_mean_TEs_4kb"
title <- "Weighted Methylation Level for long TEs (>4 kb)"

get_df_wml(list_methylRawLists_TEs_4kb, path_DB, df_name)

load_df_wml(path_DB, df_name)
```


# GWAS analysis


GWAS was performed as described in this repository: https://github.com/johanzi/gwas_gemma

The script `run_gwas_gemma.sh` can be downloaded from GitHub [gwas_gemma](https://github.com/johanzi/gwas_gemma) repository:

```{bash}
git clone https://github.com/johanzi/gwas_gemma
```


## Prepare VCF file

Check https://github.com/johanzi/gwas_gemma?tab=readme-ov-file#section-id-35

```{bash, eval=FALSE}
bcftools view -S list_accessions_to_keep.txt file.vcf.gz > subset_83.vcf

bcftools view -r Chr1,Chr2,Chr3,Chr4,Chr5 subset_80.vcf > subset_83_only_chr.vcf

bcftools view --min-ac=1 --max-alleles 2  subset_80_only_chr.vcf > subset_83_only_chr_biallelic_only_alt.vcf

vcftools --vcf subset_83_only_chr_biallelic_only_alt.vcf  \
			--minDP 3 --minGQ 25 --remove-indels --recode --recode-INFO-all \
			--out subset_83_only_chr_biallelic_only_alt_DP3_GQ25_wo_indels
```

The output file will be `subset_83_only_chr_biallelic_only_alt_DP3_GQ25_wo_singletons.recode.vcf.gz`.

## Prepare phenotye

Check https://github.com/johanzi/gwas_gemma?tab=readme-ov-file#section-id-139


## Run Gemma

The script should be executed for each context and each genomic region:

```{bash}

VCF="subset_83_only_chr_biallelic_only_alt_DP3_GQ25_wo_singletons.recode.vcf.gz"

# Genes
bash gwas_gemma/run_gwas_gemma.sh CpG_genes.tsv $VCF

bash gwas_gemma/run_gwas_gemma.sh CHG_genes.tsv $VCF

bash gwas_gemma/run_gwas_gemma.sh CHH_genes.tsv $VCF

# TEs
bash gwas_gemma/run_gwas_gemma.sh CpG_TEs.tsv $VCF

bash gwas_gemma/run_gwas_gemma.sh CHG_TEs.tsv $VCF

bash gwas_gemma/run_gwas_gemma.sh CHH_TEs.tsv $VCF

# long TEs
bash gwas_gemma/run_gwas_gemma.sh CpG_TEs_4kb.tsv $VCF

bash gwas_gemma/run_gwas_gemma.sh CHG_TEs_4kb.tsv $VCF

bash gwas_gemma/run_gwas_gemma.sh CHH_TEs_4kb.tsv $VCF

```

### GWAS genes

```{r}
dir_file="/path/to/file/"

file.name <- "CpG_genes.assoc.clean.txt"

path.file <- paste(dir_file, file.name, sep="")

SNP_significant <- GWAS_run(path.file, threshold_pvalue = "bonferroni")


file.name <- "CHG_genes.assoc.clean.txt"

path.file <- paste(dir_file, file.name, sep="")

SNP_significant <- GWAS_run(path.file, threshold_pvalue = "bonferroni")


file.name <- "CHH_genes.assoc.clean.txt"

path.file <- paste(dir_file, file.name, sep="")

SNP_significant <- GWAS_run(path.file, threshold_pvalue = "bonferroni")

```

### GWAS all TEs


```{r}
dir_file="/path/to/file/"

file.name <- "CpG_TEs.assoc.clean.txt"

path.file <- paste(dir_file, file.name, sep="")

SNP_significant <- GWAS_run(path.file, threshold_pvalue = "bonferroni")


file.name <- "CHG_TEs.assoc.clean.txt"

path.file <- paste(dir_file, file.name, sep="")

SNP_significant <- GWAS_run(path.file, threshold_pvalue = "bonferroni")


file.name <- "CHH_TEs.assoc.clean.txt"

path.file <- paste(dir_file, file.name, sep="")

SNP_significant <- GWAS_run(path.file, threshold_pvalue = "bonferroni")


```

### GWAS long TEs

```{r}

dir_file="/path/to/file/"

file.name <- "CpG_TEs_4kb.assoc.clean.txt"

path.file <- paste(dir_file, file.name, sep="")

SNP_significant <- GWAS_run(path.file, threshold_pvalue = "bonferroni")


file.name <- "CHG_TEs_4kb.assoc.clean.txt"

path.file <- paste(dir_file, file.name, sep="")

SNP_significant <- GWAS_run(path.file, threshold_pvalue = "bonferroni")


file.name <- "CHH_TEs_4kb.assoc.clean.txt"

path.file <- paste(dir_file, file.name, sep="")

SNP_significant <- GWAS_run(path.file, threshold_pvalue = "bonferroni")

```


# DMR analysis for the 3 genes



# Call allele status

```{bash, eval=FALSE}

# This script defines the step to retrieve the allele status for
# each CPV-SA accessions of the CMT2, ARABIDILLO-1, and VIM2 variants

################################################################################
# Packages/Sofwares requires
################################################################################

# vcf_melt (install with command "pip install PyVCF --user")

# bcftools

# samtools

################################################################################
########## Get VIM2 deletion, ARA1, and CMT2 status
################################################################################

# Use VCF file used for GWAS 

VCF="superVcf_19-07-04_cvis.vcf.b.gz_snps.vcf.b.gz"

#################################################################
# FBX5
#################################################################

# Get VCF data for SNP in FBX5
bcftools view -r Chr2:18513626 $VCF > Chr2_18513626.vcf

# Convert into vertical
vcf_melt Chr2_18513626.vcf > Chr2_18513626.melted.vcf 

# Keep only line with GQ >= 25 and DP >= 3
awk '$3>=25 && $4>=3 {print $0}' Chr2_18513626.melted.vcf > Chr2_18513626_GQ25_DP3.melted.vcf
 
# Accessions with alternative allele
awk '$2 == "1" {print $0}' Chr2_18513626_GQ25_DP3.melted.vcf | wc -l
84

awk '$2 == "1" {print $0}' Chr2_18513626_GQ25_DP3.melted.vcf | cut -f1 | sort - > FBX5_alt.txt
awk '$2 == "0" {print $0}' Chr2_18513626_GQ25_DP3.melted.vcf | cut -f1 | sort - > FBX5_ref.txt


awk -v OFS='\t' '{print $0,"FBX5_alt"}' FBX5_alt.txt > FBX5_alt_final.txt
awk -v OFS='\t' '{print $0,"FBX5_ref"}' FBX5_ref.txt > FBX5_ref_final.txt

cat FBX5_alt_final.txt FBX5_ref_final.txt > FBX5_allele_status.txt

rm FBX5_alt.txt FBX5_ref.txt FBX5_alt_final.txt FBX5_ref_final.txt 

#################################################################
# CMT2
#################################################################

bcftools view -r Chr4:10420088 $VCF > Chr4_10420088.vcf

# Convert into vertical
vcf_melt Chr4_10420088.vcf > Chr4_10420088.melted.vcf 

# Keep only line with GQ >= 25 and DP >= 3
awk '$3>=25 && $4>=3 {print $0}' Chr4_10420088.melted.vcf  > Chr4_10420088_GQ25_DP3.melted.vcf 

# Accessions with alternative allele
awk '$2 == "1" {print $0}' Chr4_10420088_GQ25_DP3.melted.vcf  | wc -l
65

awk '$2 == "1" {print $0}' Chr4_10420088_GQ25_DP3.melted.vcf  | cut -f1 | sort - > CMT2_alt.txt
awk '$2 == "0" {print $0}' Chr4_10420088_GQ25_DP3.melted.vcf  | cut -f1 | sort - > CMT2_ref.txt

awk -v OFS='\t' '{print $0,"CMT2_alt"}' CMT2_alt.txt > CMT2_alt_final.txt
awk -v OFS='\t' '{print $0,"CMT2_ref"}' CMT2_ref.txt > CMT2_ref_final.txt

cat CMT2_alt_final.txt CMT2_ref_final.txt > CMT2_allele_status.txt

rm CMT2_alt.txt CMT2_ref.txt CMT2_alt_final.txt CMT2_ref_final.txt 


#################################################################
# VIM2 deletion
#################################################################

# Considering that the VIM2 deletion is not present in the VCF file as it is a 
# structural variant and not a SNP, we need to assess the presence of the deletion
# based on read density at the deletion region

# coordinates of the deletion location (based on Cvi-0). The deletion is 2740 bp
# This region was defined by looking at read mapping in Cvi-0 and determine visually
# the beginning and end of the deletion
coordinates="chr1:24586731-24589471"

for i in mappedBAM/*bam; do
	name=$(basename $i | cut -d'.' -f1)
	nb_reads=$(samtools view $i $coordinates | wc -l)
	echo -e "${name}\t${nb_reads}" >> nb_reads_vim2_deletion.txt
done

cut -f2 nb_reads_vim2_deletion.txt | sort -n -
# Looking at the distribution of reads, it seems there is threshold at 14 reads. i
# Let's use 50 reads as the threshold to define that there is indeed a deletion

# Classify each sample based on nb of reads with threshold = 50
awk -v OFS="\t" '$2 <= 50 {print $1,$2,"deletion"} $2 > 50 {print $1,$2,"no_deletion"}' nb_reads_vim2_deletion.txt > nb_reads_vim2_deletion_status.txt

```


# Allele distribution by population

```{bash, eval=FALSE}
########################################################
# Summary by population in SA for VIM2, FBX5, and CMT2
#########################################################

# 4073_M (Cvi-0 is included)
clean_file="/srv/biodata/dep_coupland/grp_hancock/VCF/santos_clean_2019-07-11.txt"

# File contains 190 accessions

#########################################################
# For FBX5

while read i; do 
	grep -w $i FBX5_allele_status.txt >> FBX5_allele_status_SA.txt
done < /srv/biodata/dep_coupland/grp_hancock/VCF/santos_clean_2019-07-11.txt

while read i; do
    seqID=$(echo "$i" | cut -f1)
    name=$(python /home/zicola/SCRIPTS/find_accession/find_accession.py /home/zicola/SCRIPTS/find_accession/updated_list_seqID_name.dict $seqID | cut -f2)
	population=$(echo $name | cut -d'-' -f1)
    echo -e "${i}\t${name}\t${population}"
done < FBX5_allele_status_SA.txt > FBX5_allele_status_SA_with_names.txt


# Replace FBX5_ref by 0 and FBX5_alt by 1
sed -i 's/FBX5_ref/0/' FBX5_allele_status_SA_with_names.txt
sed -i 's/FBX5_alt/1/' FBX5_allele_status_SA_with_names.txt

# 189 retrieved, accession 12849 has no GT assigned  

cd /srv/netscratch/dep_coupland/grp_hancock/mappedBAM/CVI
samtools tview  12849.sorted.bam  -p chr2:18513626 --reference /home/zicola/TAIR10_chr_Pt_Mt/TAIR10.fasta

# Weird as many reads support the ALT allele A
cd /srv/biodata/dep_coupland/grp_hancock/johan/allele_status

# Also present after filtering but no genotype given (second column
grep "12849" Chr2_18513626_GQ25_DP3.melted.vcf
12849   .                       ['q25'] Chr2    18513626        T       [A]             1       94957   84      2003


#########################################################
# For CMT2

while read i; do 
	grep -w $i CMT2_allele_status.txt >> CMT2_allele_status_SA.txt
done < /srv/biodata/dep_coupland/grp_hancock/VCF/santos_clean_2019-07-11.txt

# 190 accessions retrieved => OK


while read i; do
    seqID=$(echo "$i" | cut -f1)
    name=$(python /home/zicola/SCRIPTS/find_accession/find_accession.py /home/zicola/SCRIPTS/find_accession/updated_list_seqID_name.dict $seqID | cut -f2)
	population=$(echo $name | cut -d'-' -f1)
    echo -e "${i}\t${name}\t${population}"
done < CMT2_allele_status_SA.txt > CMT2_allele_status_SA_with_names.txt


# Replace CMT2_ref by 0 and CMT2_alt by 1
sed -i 's/CMT2_ref/0/' CMT2_allele_status_SA_with_names.txt
sed -i 's/CMT2_alt/1/' CMT2_allele_status_SA_with_names.txt


#########################################################
# For VIM2

while read i; do 
	foo=$(grep -w $i nb_reads_vim2_deletion_all_with_name_clean.txt | cut -f1,3,4)
	population=$(echo "$foo" | cut -f3 | cut -d'-' -f1)
	echo -e "${foo}\t${population}"
done < /srv/biodata/dep_coupland/grp_hancock/VCF/santos_clean_2019-07-11.txt > VIM2_allele_status_SA_with_names.txt

# 190 accessions retrieved => OK

# Replace no_deletion by 0 and deletion by 1
sed -i 's/no_deletion/0/' VIM2_allele_status_SA_with_names.txt
sed -i 's/deletion/1/' VIM2_allele_status_SA_with_names.txt

# Integrate that with coordinates data

# Reference files
/netscratch/dep_coupland/grp_hancock/Celia/Experiments/newBronson/coord_genotype.txt

# Get first 3 first rows
cut -f1,2,3 /netscratch/dep_coupland/grp_hancock/Celia/Experiments/newBronson/coord_genotype.txt > coord_populations_SA.txt
```


# Map allele distribution by population

```{r}
#install.packages("scatterpie")
#install.packages("ggmap")
#install.packages("maps")

library(scatterpie)
library(ggmap)
library(maps)
library(plyr)

# As input file, I need longitude, latitude, population name, frequency of ancestral allele, frequency of derived allele, gene name (optional if only 1 gene), 
# the radius to define the size of the pie charts on the map (use total_individuals  * 0.005 / 10)
```

## Coordinate file 

```{r}
# Replace Cratera by SCratera in bash
# Modify header
# Data from SantoCoordinates.csv files in Google Drive Hancock lab
# There are coordinates for 31 populations

# I chose arbitrarily S11-rav1	-25.076404	17.114951 as S11 population and S24-1	-25.076925	17.105766 as S24
# population, remove the other S11 and S24 from the file

df_coordinates <- read.table("data/coord_populations_SA.txt", header=TRUE)
names(df_coordinates) <- c("population","long","lat")
```

## VIM2 distribution

```{r}

df_vim2 <- read.table("data/VIM2_allele_status_SA_with_names.txt", header=FALSE)
names(df_vim2) <- c("seqID","GT","accession","population")

# Summarize by population
df_vim2_summary <- ddply(df_vim2, .(population), summarise, total=length(population), freq_derived=sum(GT)/total, freq_derived=(sum(GT)/total), freq_ancestral=(1-freq_derived), radius=(total*0.005/10))

# Merge with coordinate, somehow I get duplicated rows. Use unique()
df_vim2_coord <- unique(merge(df_vim2_summary, df_coordinates, by="population"))

world <- map_data("world")
SA <- world[world$long > -30 & world$long < -20 & world$lat > 10 & world$lat < 20,]

ggplot(SA, aes(long, lat)) +
  geom_map(map = world, aes(map_id = region), fill = NA, color = "black") +
  coord_quickmap() +
  xlim(-25.1, -25.01) +
  ylim(17.09, 17.135) +
  theme(plot.background = element_rect(fill = "transparent"), panel.background = element_rect(fill = "transparent"), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.line.x = element_line(color = "black", size = 0.5), axis.line.y = element_line(color = "black", size = 0.5)) +
  xlab("Longitude") +
  ylab("Latitude") +
  geom_scatterpie(aes(x = long, y = lat, r = radius, group = population), data = df_vim2_coord, cols = c("freq_derived","freq_ancestral"), sorted_by_radius = TRUE, alpha = .5) +
  labs(fill = "VIM2 alleles") +
  scale_fill_manual(values = c("coral2", "turquoise4")) +
  theme(axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16)) +
  geom_scatterpie_legend(df_vim2_coord$radius, x = -25.03, y = 17.124, n = 4, labeller = function(x) 10 * x / 0.005)

```


## CMT2 distribution

```{r}

df_cmt2 <- read.table("data/CMT2_allele_status_SA_with_names.txt", header=FALSE)
names(df_cmt2) <- c("seqID","GT","accession","population")

# Summarize by population
df_cmt2_summary <- ddply(df_cmt2, .(population), summarise, total=length(population), freq_derived=(sum(GT)/total), freq_ancestral=(1-freq_derived), radius=(total*0.005/10))

# Merge with coordinate, somehow I get duplicated rows. Use unique()
df_cmt2_coord <- merge(df_cmt2_summary, df_coordinates, by="population")

world <- map_data("world")
SA <- world[world$long > -30 & world$long < -20 & world$lat > 10 & world$lat < 20,]

ggplot(SA, aes(long, lat)) +
  geom_map(map = world, aes(map_id = region), fill = NA, color = "black") +
  coord_quickmap() +
  xlim(-25.1, -25.01) +
  ylim(17.09, 17.135) +
  theme(plot.background = element_rect(fill = "transparent"), panel.background = element_rect(fill = "transparent"), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.line.x = element_line(color = "black", size = 0.5), axis.line.y = element_line(color = "black", size = 0.5)) +
  xlab("Longitude") +
  ylab("Latitude") +
  geom_scatterpie(aes(x = long, y = lat, r = radius, group = population), data = df_cmt2_coord, cols = c("freq_derived","freq_ancestral"), sorted_by_radius = TRUE, alpha = .5) +
  labs(fill = "CMT2 alleles") +
  scale_fill_manual(values = c("turquoise4", "coral2")) +
  theme(axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16)) +
  geom_scatterpie_legend(df_cmt2_coord$radius, x = -25.03, y = 17.124, n = 4, labeller = function(x) 10 * x / 0.005)

  
```

## FBX5 distribution

```{r}

df_fbx5 <- read.table("data/FBX5_allele_status_SA_with_names.txt", header=FALSE)
names(df_fbx5) <- c("seqID","GT","accession","population")

# Summarize by population
df_fbx5_summary <- ddply(df_fbx5, .(population), summarise, total=length(population), freq_derived=(sum(GT)/total), freq_ancestral=(1-freq_derived), radius=(total*0.005/10))

# Merge with coordinate, somehow I get duplicated rows. Use unique()
df_fbx5_coord <- unique(merge(df_fbx5_summary, df_coordinates, by="population"))

world <- map_data("world")
SA <- world[world$long > -30 & world$long < -20 & world$lat > 10 & world$lat < 20,]

ggplot(SA, aes(long, lat)) +
  geom_map(map = world, aes(map_id = region), fill = NA, color = "black") +
  coord_quickmap() +
  xlim(-25.1, -25.01) +
  ylim(17.09, 17.135) +
  theme(plot.background = element_rect(fill = "transparent"), panel.background = element_rect(fill = "transparent"), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.line.x = element_line(color = "black", size = 0.5), axis.line.y = element_line(color = "black", size = 0.5)) +
  xlab("Longitude") +
  ylab("Latitude") +
  geom_scatterpie(aes(x = long, y = lat, r = radius, group = population), data = df_fbx5_coord, cols = c("freq_derived","freq_ancestral"), sorted_by_radius = TRUE, alpha = .5) +
  labs(fill = "FBX5 alleles") +
  scale_fill_manual(values = c("coral2","turquoise4")) +
  theme(axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16)) +
  geom_scatterpie_legend(df_fbx5_coord$radius, x = -25.03, y = 17.124, n = 4, labeller = function(x) 10 * x / 0.005)

```


## Plot diagram VIM2

```{r}
df_vim2_coord$population_nb <- paste(df_vim2_coord$population, " n=", df_vim2_coord$total, sep="")

# Remove Cvi
df <- df_vim2_coord[!(df_vim2_coord$population=="Cvi"),]

# Order accesstion by longitude
df$population_nb <- factor(df$population_nb, levels = df$population_nb[order(df$long)])

ggplot(data = df, aes(population_nb, freq_derived)) +
  ggtitle("VIM2 deletion frequency") +
  geom_bar(aes(x = population_nb, y = freq_derived), stat = "identity", colour = "black", fill = "grey") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Deletion frequency") +
  xlab("Population") +
  scale_y_continuous(labels = scales::percent)


```

## Plot diagram CMT2

```{r}

df_cmt2_coord$population_nb <- paste(df_cmt2_coord$population, " n=", df_cmt2_coord$total, sep="")

# Remove Cvi
df <- df_cmt2_coord[!(df_cmt2_coord$population=="Cvi"),]

# Order accesstion by longitude
df$population_nb <- factor(df$population_nb, levels = df$population_nb[order(df$long)])

ggplot(data = df, aes(population_nb, freq_derived)) +
  ggtitle("Derived CMT2 allele frequency") +
  geom_bar(aes(x = population_nb, y = freq_derived), stat = "identity", colour = "black", fill = "grey") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Derived allele frequency") +
  xlab("Population") +
  scale_y_continuous(labels = scales::percent)


```



## Plot diagram FBX5

```{r}
# Create new variable which concatenat population name and number of accessions
df_fbx5_coord$population_nb <- paste(df_fbx5_coord$population, " n=", df_fbx5_coord$total, sep="")

# Remove Cvi
df <- df_fbx5_coord[!(df_fbx5_coord$population=="Cvi"),]

# Order accesstion by longitude
df$population_nb <- factor(df$population_nb, levels = df$population_nb[order(df$long)])

ggplot(data = df, aes(population_nb, freq_derived)) +
  ggtitle("Derived ARABIDILLO-1 allele frequency") +
  geom_bar(aes(x = population_nb, y = freq_derived), stat = "identity", colour = "black", fill = "grey") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Derived allele frequency") +
  xlab("Population") +
  scale_y_continuous(labels = scales::percent)

```












